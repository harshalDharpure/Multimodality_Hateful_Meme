{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOZPT9xkTgl3NEup/45Amdt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshalDharpure/Multimodality_Hateful_Meme/blob/main/Bias_weight_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mf7aNLasHaKs"
      },
      "outputs": [],
      "source": [
        "#Modified for Hindi datasets Prompthate code"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import config\n",
        "import os\n",
        "from train import train_for_epoch, WeightedBCELoss  # Import WeightedBCELoss\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import RobertaTokenizer\n",
        "\n",
        "# Import a function to compute bias weight (you should implement this function)\n",
        "from utils import compute_bias_weight\n",
        "\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    opt = config.parse_opt()\n",
        "    torch.cuda.set_device(opt.CUDA_DEVICE)\n",
        "    set_seed(opt.SEED)\n",
        "\n",
        "    # Create tokenizer\n",
        "    tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
        "\n",
        "    constructor = 'build_baseline'\n",
        "    if opt.MODEL == 'pbm':\n",
        "        from dataset import Multimodal_Data\n",
        "        import baseline\n",
        "        train_set = Multimodal_Data(opt, tokenizer, opt.DATASET, 'train', opt.SEED - 1111)\n",
        "        test_set = Multimodal_Data(opt, tokenizer, opt.DATASET, 'test')\n",
        "        label_list = [train_set.label_mapping_id[i] for i in train_set.label_mapping_word.keys()]\n",
        "        model = getattr(baseline, constructor)(opt, label_list).cuda()\n",
        "    else:\n",
        "        from roberta_dataset import Roberta_Data\n",
        "        import roberta_baseline\n",
        "        train_set = Roberta_Data(opt, tokenizer, opt.DATASET, 'train', opt.SEED - 1111)\n",
        "        test_set = Roberta_Data(opt, tokenizer, opt.DATASET, 'test')\n",
        "        model = getattr(roberta_baseline, constructor)(opt).cuda()\n",
        "\n",
        "    train_loader = DataLoader(train_set, opt.BATCH_SIZE, shuffle=True, num_workers=1)\n",
        "    test_loader = DataLoader(test_set, opt.BATCH_SIZE, shuffle=False, num_workers=1)\n",
        "\n",
        "    # Compute bias weight based on your evaluation (implement compute_bias_weight)\n",
        "    if opt.MODEL == 'pbm':\n",
        "        label_distribution = train_set.get_label_distribution()\n",
        "        bias_weight = compute_bias_weight(label_distribution)\n",
        "    else:\n",
        "        bias_weight = None\n",
        "\n",
        "    train_for_epoch(opt, model, train_loader, test_loader, bias_weight)  # Pass bias_weight\n",
        "\n",
        "    exit(0)\n"
      ],
      "metadata": {
        "id": "fNKb9bx3JLGI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}