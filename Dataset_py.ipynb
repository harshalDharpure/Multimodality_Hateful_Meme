{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPoQUOUxOBQ06iEmL7l5Qxd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshalDharpure/Multimodality_Hateful_Meme/blob/main/Dataset_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mf7aNLasHaKs"
      },
      "outputs": [],
      "source": [
        "#Modified for Hindi datasets Prompthate code"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pickle as pkl\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "class Multimodal_Data:\n",
        "    def __init__(self, opt, tokenizer, dataset, mode='train', few_shot_index=0):\n",
        "        super(Multimodal_Data, self).__init__()\n",
        "        self.opt = opt\n",
        "        self.tokenizer = tokenizer\n",
        "        self.mode = mode\n",
        "        if self.opt.FEW_SHOT:\n",
        "            self.few_shot_index = str(few_shot_index)\n",
        "            self.num_shots = self.opt.NUM_SHOTS\n",
        "            print('Few shot learning setting for Iteration:', self.few_shot_index)\n",
        "            print('Number of shots:', self.num_shots)\n",
        "\n",
        "        self.num_ans = self.opt.NUM_LABELS\n",
        "        # Maximum length for a single sentence\n",
        "        self.length = self.opt.LENGTH\n",
        "        # Maximum length of the concatenation of sentences\n",
        "        self.total_length = self.opt.TOTAL_LENGTH\n",
        "        self.num_sample = self.opt.NUM_SAMPLE\n",
        "        self.add_ent = self.opt.ADD_ENT\n",
        "        self.add_dem = self.opt.ADD_DEM\n",
        "        print('Adding entity information?', self.add_ent)\n",
        "        print('Adding demographic information?', self.add_dem)\n",
        "\n",
        "        if self.opt.FINE_GRIND:\n",
        "            self.label_mapping_word = {0: self.opt.POS_WORD, 1: self.opt.NEG_WORD}\n",
        "        else:\n",
        "            self.label_mapping_word = {0: self.opt.POS_WORD, 1: self.opt.NEG_WORD}\n",
        "\n",
        "        self.label_mapping_id = {}\n",
        "        for label in self.label_mapping_word.keys():\n",
        "            mapping_word = self.label_mapping_word[label]\n",
        "            self.label_mapping_id[label] = self.tokenizer._convert_token_to_id(\n",
        "                self.tokenizer.tokenize(' ' + mapping_word)[0])\n",
        "            print('Mapping for label %d, word %s, index %d' % (label, mapping_word, self.label_mapping_id[label]))\n",
        "\n",
        "        self.template = \"*<s>**sent_0*.*_It_was*label_**</s>*\"\n",
        "        self.template_list = self.template.split('*')\n",
        "        print('Template:', self.template)\n",
        "        print('Template list:', self.template_list)\n",
        "\n",
        "        self.special_token_mapping = {\n",
        "            '<s>': self.tokenizer.convert_tokens_to_ids('<s>'),\n",
        "            '<mask>': self.tokenizer.mask_token_id,\n",
        "            '<pad>': self.tokenizer.pad_token_id,\n",
        "            '</s>': self.tokenizer.convert_tokens_to_ids('<\\s>')\n",
        "        }\n",
        "\n",
        "        if self.opt.DEM_SAMP:\n",
        "            print('Using demonstration sampling strategy...')\n",
        "            self.img_rate = self.opt.IMG_RATE\n",
        "            self.text_rate = self.opt.TEXT_RATE\n",
        "            self.samp_rate = self.opt.SIM_RATE\n",
        "            print('Image rate for measuring CLIP similarity:', self.img_rate)\n",
        "            print('Text rate for measuring CLIP similarity:', self.text_rate)\n",
        "            print('Sampling from top:', self.samp_rate * 100.0, 'examples')\n",
        "            self.clip_clean = self.opt.CLIP_CLEAN\n",
        "            clip_path = os.path.join(self.opt.CAPTION_PATH, dataset, dataset + '_sim_scores.pkl')\n",
        "            print('Clip feature path:', clip_path)\n",
        "            self.clip_feature = pkl.load(open(clip_path, 'rb'))\n",
        "\n",
        "        self.support_examples = self.load_entries('train')\n",
        "        print('Length of supporting examples:', len(self.support_examples))\n",
        "        self.entries = self.load_entries(mode)\n",
        "        if self.opt.DEBUG:\n",
        "            self.entries = self.entries[:128]\n",
        "        self.prepare_exp()\n",
        "        print('The length of the dataset for:', mode, 'is:', len(self.entries))\n",
        "\n",
        "    def load_entries(self, mode):\n",
        "        path = os.path.join(self.opt.DATA, 'domain_splits', self.opt.DATASET + '_' + mode + '.json')\n",
        "        data = json.load(open(path, 'rb'))\n",
        "        cap_path = os.path.join(self.opt.CAPTION_PATH, self.opt.DATASET + '_' + self.opt.PRETRAIN_DATA,\n",
        "                               self.opt.IMG_VERSION + '_captions.pkl')\n",
        "        captions = pkl.load(open(cap_path, 'rb'))\n",
        "        entries = []\n",
        "        for k, row in enumerate(data):\n",
        "            label = row['label']\n",
        "            img = row['img']\n",
        "            cap = captions[img.split('.')[0]][:-1]  # remove the punctuation at the end\n",
        "            sent = row['clean_sent']\n",
        "            # remember the punctuations at the end of each sentence\n",
        "            cap = cap + ' . ' + sent + ' . '\n",
        "            # whether using external knowledge\n",
        "            if self.add_ent:\n",
        "                cap = cap + ' . ' + row['entity'] + ' . '\n",
        "            if self.add_dem:\n",
        "                cap = cap + ' . ' + row['race'] + ' . '\n",
        "            entry = {\n",
        "                'cap': cap.strip(),\n",
        "                'label': label,\n",
        "                'img': img\n",
        "            }\n",
        "            entries.append(entry)\n",
        "        return entries\n",
        "\n",
        "    def enc(self, text):\n",
        "        return self.tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "    def prepare_exp(self):\n",
        "        support_indices = list(range(len(self.support_examples))\n",
        "        self.example_idx = []\n",
        "        for sample_idx in tqdm(range(self.num_sample)):\n",
        "            for query_idx in range(len(self.entries)):\n",
        "                if self.opt.DEM_SAMP:\n",
        "                    candidates = [support_idx for support_idx in support_indices\n",
        "                                  if support_idx != query_idx or self.mode != \"train\"]\n",
        "                    sim_score = []\n",
        "                    count_each_label = {label: 0 for label in range(self.opt.NUM_LABELS}\n",
        "                    context_indices = []\n",
        "                    clip_info_que = self.clip_feature[self.entries[query_idx]['img']]\n",
        "                    for support_idx in candidates:\n",
        "                        img = self.support_examples[support_idx]['img']\n",
        "                        if self.clip_clean:\n",
        "                            img_sim = clip_info_que['clean_img'][img]\n",
        "                        else:\n",
        "                            img_sim = clip_info_que['img'][img]\n",
        "                        text_sim = clip_info_que['text'][img]\n",
        "                        total_sim = self.img_rate * img_sim + self.text_rate * text_sim\n",
        "                        sim_score.append((support_idx, total_sim))\n",
        "                    sim_score.sort(key=lambda x: x[1], reverse=True)\n",
        "                    num_valid = int(len(sim_score) // self.opt.NUM_LABELS * self.samp_rate)\n",
        "                    for support_idx, score in sim_score:\n",
        "                        cur_label = self.support_examples[support_idx]['label']\n",
        "                        if count_each_label[cur_label] < num_valid:\n",
        "                            count_each_label[cur_label] += 1\n",
        "                            context_indices.append(support_idx)\n",
        "                else:\n",
        "                    context_indices = [support_idx for support_idx in support_indices\n",
        "                                       if support_idx != query_idx or self.mode != \"train\"]\n",
        "                self.example_idx.append((query_idx, context_indices, sample_idx))\n",
        "\n",
        "    def select_context(self, context_examples):\n",
        "        max_demo_per_label = 1\n",
        "        counts = {k: 0 for k in range(self.opt.NUM_LABELS)}\n",
        "        if self.opt.DEBUG:\n",
        "            print('Number of context examples available:', len(context_examples))\n",
        "        order = np.random.permutation(len(context_examples))\n",
        "        selection = []\n",
        "        for i in order:\n",
        "            label = context_examples[i]['label']\n",
        "            if num_labels == 1:\n",
        "                # Regression\n",
        "                #No implementation currently\n",
        "                label = '0' if\\\n",
        "                float(label) <= median_mapping[self.args.task_name] else '1'\n",
        "                        if counts[label] < max_demo_per_label:\n",
        "                selection.append(context_examples[i])\n",
        "                counts[label] += 1\n",
        "             if sum(counts.values()) == len(counts) * max_demo_per_label:\n",
        "                break\n",
        "\n",
        "        assert len(selection) > 0\n",
        "        return selection\n",
        "\n",
        "    def process_prompt(self, examples, first_sent_limit, other_sent_limit):\n",
        "        if self.fine_grind:\n",
        "            prompt_arch = ' It was targeting '\n",
        "        else:\n",
        "            prompt_arch = ' It was '\n",
        "        input_ids = []\n",
        "        attention_mask = []\n",
        "        mask_pos = None  # Position of the mask token\n",
        "        concat_sent = \"\"\n",
        "        for segment_id, ent in enumerate(examples):\n",
        "            new_tokens = []\n",
        "            if segment_id == 0:\n",
        "                new_tokens.append(self.special_token_mapping['<s>'])\n",
        "                length = first_sent_limit\n",
        "                temp = prompt_arch + '<mask>' + ' . </s>'\n",
        "            else:\n",
        "                length = other_sent_limit\n",
        "                if self.fine_grind:\n",
        "                    label_word = self.label_mapping_word[ent['label']]\n",
        "                else:\n",
        "                    label_word = self.label_mapping_word[ent['label']]\n",
        "                temp = prompt_arch + label_word + ' . </s>'\n",
        "            new_tokens += self.enc(' ' + ent['cap'])\n",
        "            new_tokens = new_tokens[:length]\n",
        "            new_tokens += self.enc(temp)\n",
        "            whole_sent = ' ' + ent['cap'] + temp\n",
        "            concat_sent += whole_sent\n",
        "\n",
        "            input_ids += new_tokens\n",
        "            attention_mask += [1 for i in range(len(new_tokens)]\n",
        "\n",
        "        while len(input_ids) < self.total_length:\n",
        "            input_ids.append(self.special_token_mapping['<pad>'])\n",
        "            attention_mask.append(0)\n",
        "        if len(input_ids) > self.total_length:\n",
        "            input_ids = input_ids[:self.total_length]\n",
        "            attention_mask = attention_mask[:self.total_length]\n",
        "        mask_pos = [input_ids.index(self.special_token_mapping['<mask>'])]\n",
        "\n",
        "        assert mask_pos[0] < self.total_length\n",
        "        result = {'input_ids': input_ids,\n",
        "                  'sent': '<s>' + concat_sent,\n",
        "                  'attention_mask': attention_mask,\n",
        "                  'mask_pos': mask_pos}\n",
        "        return result\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        entry = self.entries[index]\n",
        "        query_idx, context_indices, bootstrap_idx = self.example_idx[index]\n",
        "        supports = self.select_context([self.support_examples[i] for i in context_indices])\n",
        "        exps = []\n",
        "        exps.append(entry)\n",
        "        exps.extend(supports)\n",
        "        prompt_features = self.process_prompt(\n",
        "            exps,\n",
        "            self.length,\n",
        "            self.length\n",
        "        )\n",
        "\n",
        "        vid = entry['img']\n",
        "        label = torch.tensor(entry['label'])\n",
        "        target = torch.from_numpy(np.zeros((self.num_ans), dtype=np.float32))\n",
        "        target[label] = 1.0\n",
        "\n",
        "        cap_tokens = torch.Tensor(prompt_features['input_ids'])\n",
        "        mask_pos = torch.LongTensor(prompt_features['mask_pos'])\n",
        "        mask = torch.Tensor(prompt_features['attention_mask'])\n",
        "        batch = {\n",
        "            'sent': prompt_features['sent'],\n",
        "            'mask': mask,\n",
        "            'img': vid,\n",
        "            'target': target,\n",
        "            'cap_tokens': cap_tokens,\n",
        "            'mask_pos': mask_pos,\n",
        "            'label': label\n",
        "        }\n",
        "        return batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.entries)\n"
      ],
      "metadata": {
        "id": "fNKb9bx3JLGI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}