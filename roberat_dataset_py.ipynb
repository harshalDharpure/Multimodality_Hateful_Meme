{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMtp5EgRMsjyvQAew9Iuuav",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshalDharpure/Multimodality_Hateful_Meme/blob/main/roberat_dataset_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mf7aNLasHaKs"
      },
      "outputs": [],
      "source": [
        "#Modified for Hindi datasets Prompthate code"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import RobertaTokenizer\n",
        "from torch.utils.data import Dataset\n",
        "import config\n",
        "import random\n",
        "\n",
        "class RobertaDataset(Dataset):\n",
        "    def __init__(self, opt, dataset, mode='train', few_shot_index=0):\n",
        "        self.opt = opt\n",
        "        self.tokenizer = RobertaTokenizer.from_pretrained(opt.MODEL_NAME)\n",
        "        self.mode = mode\n",
        "        self.dataset = dataset\n",
        "        self.num_ans = opt.NUM_LABELS\n",
        "        self.unimodal = opt.UNIMODAL\n",
        "\n",
        "        if opt.FEW_SHOT:\n",
        "            self.few_shot_index = str(few_shot_index)\n",
        "            self.num_shots = opt.NUM_SHOTS\n",
        "            print('Few shot learning setting for Iteration:', self.few_shot_index)\n",
        "            print('Number of shots:', self.num_shots)\n",
        "\n",
        "        self.length = opt.LENGTH\n",
        "\n",
        "        self.entries = self.load_entries(mode)\n",
        "        if opt.DEBUG:\n",
        "            self.entries = self.entries[:128]\n",
        "\n",
        "    def load_entries(self, mode):\n",
        "        path = os.path.join(self.opt.DATA, 'domain_splits', f'{self.opt.DATASET}_{mode}.json')\n",
        "        data = json.load(open(path, 'rb'))\n",
        "        captions_path = os.path.join(self.opt.CAPTION_PATH, f'{self.opt.DATASET}_{self.opt.PRETRAIN_DATA}_{self.opt.IMG_VERSION}_captions.pkl')\n",
        "        captions = self.load_pkl(captions_path)\n",
        "\n",
        "        entries = []\n",
        "\n",
        "        for row in data:\n",
        "            label = row['label']\n",
        "            img = row['img']\n",
        "            cap = captions.get(img.split('.')[0], '')[:-1]  # remove the punctuation in the end\n",
        "            sent = row['clean_sent']\n",
        "\n",
        "            if not self.unimodal:\n",
        "                cap = cap + ' . ' + sent\n",
        "            else:\n",
        "                cap = cap + ' . ' + sent + ' . '\n",
        "\n",
        "            entry = {\n",
        "                'cap': cap.strip(),\n",
        "                'label': label,\n",
        "                'img': img\n",
        "            }\n",
        "            entries.append(entry)\n",
        "\n",
        "        return entries\n",
        "\n",
        "    def process_tokens(self, sent):\n",
        "        tokens = self.tokenizer.encode(sent, add_special_tokens=True, max_length=self.length, truncation=True, padding='max_length', return_attention_mask=True)\n",
        "        return tokens\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        entry = self.entries[index]\n",
        "        vid = entry['img']\n",
        "        label = torch.tensor(entry['label'])\n",
        "        target = torch.zeros(self.num_ans, dtype=torch.float32)\n",
        "        target[label] = 1.0\n",
        "        tokens = self.process_tokens(entry['cap'])\n",
        "        tokens = torch.Tensor(tokens)\n",
        "\n",
        "        batch = {\n",
        "            'img': vid,\n",
        "            'cap_tokens': tokens,\n",
        "            'label': label,\n",
        "            'target': target\n",
        "        }\n",
        "\n",
        "        if not self.unimodal:\n",
        "            info = self.load_img_features(vid)\n",
        "            feat = torch.from_numpy(info['features'])\n",
        "            batch['feat'] = feat\n",
        "\n",
        "        return batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.entries)\n",
        "\n",
        "    def load_pkl(self, path):\n",
        "        with open(path, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "        return data\n",
        "\n",
        "    def load_img_features(self, vid):\n",
        "        if self.dataset == 'mem':\n",
        "            path = os.path.join(self.opt.DATA, 'multimodal-hate', 'mem', 'faster_hatefulmem_clean_36', vid.split('.')[0] + '.npy')\n",
        "        else:\n",
        "            path = os.path.join(self.opt.DATA, 'multimodal-hate', 'harm', 'clean_features', vid.split('.')[0] + '.npy')\n",
        "        info = np.load(path, allow_pickle=True).item()\n",
        "        return info\n"
      ],
      "metadata": {
        "id": "fNKb9bx3JLGI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}